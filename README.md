# Zero Waste GRPO

**Zero Waste GRPO** is a memory-efficient and scalable implementation of GRPO, designed for multi-node training on GH200 systems.

## Highlights

- âš™ï¸ Supports **DeepSpeed ZeRO-1/2/3** with fine-grained CPU offloading
- ðŸ” Automatic checkpointing and **job resubmission** before timeout
- ðŸ§  Reference model offloading with dynamic reloading for loss computation
- ðŸš€ Built-in support for **vLLM**-based sampling and inference
- ðŸ§ª Experimental integration with **Transformer Engine FP8 inference**
- ðŸ“ˆ Seamless **WandB tracking** across restarts

## Structure
```
GRPO/
â”œâ”€â”€ configs/ # DeepSpeed + model config files
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ grpo/ # Core training logic
â”‚ â”œâ”€â”€ misc/ # FP8 utilities
â”‚ â”œâ”€â”€ utils/ # Shared memory, tracker, SLURM utils
â”‚ â””â”€â”€ vllm/ # vLLM client, server, inference wrapper
â”œâ”€â”€ rewards.py # Task-specific reward function
â”œâ”€â”€ train_rl.py # Main RL training driver script
â”œâ”€â”€ vllm_server.py # Hackable FastAPI Server
â”œâ”€â”€ Dockerfile # Includes everything needed to run the code
```

## Setup

Before running, create a `.env` file in the root directory with:
```
HF_TOKEN=your_huggingface_token
WANDB_API_KEY=your_wandb_key
```

In order to allow resubmission you should also run this once:
```
mkdir -p $HOME/.slurm_env
env | sort > $HOME/.slurm_env/login_env.txt
```

## Usage

On Clariden:
```
sbatch grpo.sh
```

Otherwise:
```
./start_rl_training.sh --config-name qwen14b.yaml \
    --config_file configs/deepspeed/zero3.yaml
```
